{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Totaljobs/CWjobs Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_file = 'data/raw/totaljobs.csv'\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/ocean_jobs/data/spiders/totaljobs.py\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a src/ocean_jobs/data/spiders/totaljobs.py\n",
    "\n",
    "class JobSpider(scrapy.Spider):\n",
    "    name = \"jobs\"\n",
    "    allowed_domains = ['www.cwjobs.co.uk', 'www.totaljobs.com']\n",
    "    start_urls = []\n",
    "    NUM_PAGES = 5\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://www.cwjobs.co.uk/jobs/data-scientist/in-london?radius=10&s=header',\n",
    "            'https://www.totaljobs.com/jobs/data-scientist/in-london?radius=10'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse1)\n",
    "            \n",
    "\n",
    "    def parse1(self, response):\n",
    "        page_num = response.css(\".results-footer-links-container nav span.active::text\").get()\n",
    "        print(page_num)\n",
    "        next_page = response.url.split(\"?\")[0]+response.css(\".results-footer-links-container nav a.btn.btn-default.next::attr(href)\").extract()[0]\n",
    "        \n",
    "        if int(page_num) <= 5:\n",
    "            \n",
    "            \n",
    "            for job in response.css(\"div.job-title\"):\n",
    "                yield scrapy.Request(url=str(job.css(\"a::attr(href)\").get()), callback=self.parse2)\n",
    "                \n",
    "            yield scrapy.Request(url=next_page, callback=self.parse1)\n",
    "            \n",
    "    def parse2(self, response):\n",
    "        \n",
    "        company = response.css(\"section.job-summary li[class='company icon'] div a::text\").get()\n",
    "\n",
    "        description = BeautifulSoup(\"\".join(response.css(\"div.job-description p\").getall()), 'html.parser')\n",
    "        \n",
    "        salary_details = response.css(\"section.job-summary li[class='salary icon'] div::text\").get()\n",
    "        \n",
    "        job_type = response.css(\"section.job-summary li[class='job-type icon'] div::text\").get()\n",
    "        \n",
    "        yield {\"URL\": response.url, \"Company-Recruiter\": company, \"Description\": description.get_text(), \"Salary\": salary_details, \"Job_Type\": job_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/ocean_jobs/data/crawl_totaljobs.py\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "# from ocean_jobs.data.spiders.totaljobs import JobSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a src/ocean_jobs/data/crawl_totaljobs.py\n",
    "\n",
    "def run_crawler(output_file):\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'FEED_FORMAT': 'csv',\n",
    "        'FEED_URI': output_file\n",
    "    })\n",
    "    process.crawl(JobSpider)\n",
    "    process.start() # the script will block here until the crawling is finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a src/ocean_jobs/data/crawl_totaljobs.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_file = 'data/raw/totaljobs.csv'\n",
    "    run_crawler(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
